{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a62acba",
   "metadata": {},
   "source": [
    "Objective: get_history should fetch all the data at once then save it to separate files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b678491",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Historic_Crypto import HistoricalData\n",
    "from audiolizer import granularity, audiolizer_temp_dir\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "granularity = int(os.environ.get('AUDIOLIZER_GRANULARITY', 300)) # seconds\n",
    "audiolizer_temp_dir = os.environ.get('AUDIOLIZER_TEMP', './history/')\n",
    "print('audiolizer temp data:', audiolizer_temp_dir)\n",
    "\n",
    "\n",
    "def refactor(df, frequency='1W'):\n",
    "    \"\"\"Refactor/rebin the data to a lower cadence\n",
    "\n",
    "    The data is regrouped using pd.Grouper\n",
    "    \"\"\"\n",
    "    low = df.low.groupby(pd.Grouper(freq=frequency)).min()\n",
    "    high = df.high.groupby(pd.Grouper(freq=frequency)).max()\n",
    "    close = df.close.groupby(pd.Grouper(freq=frequency)).last()\n",
    "    open_ = df.open.groupby(pd.Grouper(freq=frequency)).first()\n",
    "    volume = df.volume.groupby(pd.Grouper(freq=frequency)).sum()\n",
    "    return pd.DataFrame(dict(low=low, high=high, open=open_, close=close, volume=volume))\n",
    "\n",
    "\n",
    "def load_date(ticker, granularity, int_):\n",
    "    start_ = int_.left.strftime('%Y-%m-%d-%H-%M')\n",
    "    end_ = int_.right.strftime('%Y-%m-%d-%H-%M')\n",
    "    try:\n",
    "        return HistoricalData(ticker,\n",
    "                              granularity,\n",
    "                              start_,\n",
    "                              end_,\n",
    "                              ).retrieve_data()\n",
    "    except:\n",
    "        print('could not load using {} {}'.format(start_, end_))\n",
    "        raise\n",
    "\n",
    "\n",
    "def get_gaps(df, granularity):\n",
    "    new_ = refactor(df, '{}s'.format(granularity))\n",
    "    return new_[new_.close.isna()]\n",
    "\n",
    "\n",
    "def fetch_data(ticker, granularity, start_, end_):\n",
    "    \"\"\"Need dates in this format %Y-%m-%d-%H-%M\"\"\"\n",
    "    try:\n",
    "        return HistoricalData(ticker,\n",
    "                              granularity,\n",
    "                              start_,\n",
    "                              end_,\n",
    "                              ).retrieve_data()\n",
    "    except:\n",
    "        print('could not load using {} {}'.format(start_, end_))\n",
    "        raise\n",
    "\n",
    "\n",
    "def write_data(df, ticker):\n",
    "    for t, day in df.groupby(pd.Grouper(freq='1D')):\n",
    "        tstr = t.strftime('%Y-%m-%d-%H-%M')\n",
    "        fname = audiolizer_temp_dir + '/{}-{}.csv.gz'.format(\n",
    "                ticker, t.strftime('%Y-%m-%d'))\n",
    "        if len(day) > 1:\n",
    "            day.to_csv(fname, compression='gzip')\n",
    "            print('wrote {}'.format(fname))\n",
    "        \n",
    "def fetch_missing(files_status, ticker, granularity):\n",
    "    \"\"\"Iterate over batches of missing dates\"\"\"\n",
    "    for batch, g in files_status[files_status.found==0].groupby('batch', sort=False):\n",
    "        t1, t2 = g.iloc[[0, -1]].index\n",
    "        # extend by 1 day whether or not t1 == t2\n",
    "        t2 += pd.Timedelta('1D')\n",
    "        endpoints = [t.strftime('%Y-%m-%d-%H-%M') for t in [t1, t2]]\n",
    "        print('fetching {}, {}'.format(len(g), endpoints))\n",
    "        df = fetch_data(ticker, granularity, *endpoints)\n",
    "        write_data(df, ticker)\n",
    "\n",
    "def get_history(ticker, start_date, end_date = None, granularity=granularity):\n",
    "    \"\"\"Fetch/load historical data from Coinbase API at specified granularity\n",
    "    \n",
    "    Data loaded from start_date through end of end_date\n",
    "    params:\n",
    "        start_date: (str) (see pandas.to_datetime for acceptable formats)\n",
    "        end_date: (str)\n",
    "        granularity: (int) seconds (default: 300)\n",
    "\n",
    "    price data is saved by ticker and date and stored in audiolizer_temp_dir\n",
    "    \"\"\"\n",
    "    start_date = pd.to_datetime(start_date).tz_localize(None)\n",
    "    \n",
    "    today = pd.Timestamp.now().tz_localize(None)\n",
    "    if end_date is None:\n",
    "        end_date = today + pd.Timedelta('1D')\n",
    "    else:\n",
    "        end_date = min(today, pd.to_datetime(end_date).tz_localize(None)) + pd.Timedelta('1D')\n",
    "        \n",
    "    fnames = []\n",
    "    foundlings = []\n",
    "    dates = []\n",
    "    batch = []\n",
    "    batch_number = 0\n",
    "    last_found = -1\n",
    "    for int_ in pd.interval_range(start_date, end_date):\n",
    "        dates.append(int_.left)\n",
    "        fname = audiolizer_temp_dir + '/{}-{}.csv.gz'.format(\n",
    "            ticker, int_.left.strftime('%Y-%m-%d'))\n",
    "        found = int(os.path.exists(fname))\n",
    "        foundlings.append(found)\n",
    "        if found != last_found:\n",
    "            batch_number += 1\n",
    "        last_found = found\n",
    "        batch.append(batch_number)\n",
    "        fnames.append(fname)\n",
    "    \n",
    "    \n",
    "    files_status = pd.DataFrame(dict(files=fnames, found=foundlings, batch=batch), index=dates)\n",
    "    fetch_missing(files_status, ticker, granularity)\n",
    "\n",
    "    df = pd.concat(map(lambda file: pd.read_csv(file, index_col='time', parse_dates=True),\n",
    "                         fnames)).drop_duplicates()\n",
    "    gaps = get_gaps(df, granularity)\n",
    "\n",
    "    if len(gaps) > 0:\n",
    "        print('found {} data gaps'.format(len(gaps)))\n",
    "        # fetch the data for each date\n",
    "        for start_date in gaps.groupby(pd.Grouper(freq='1d')).first().index:\n",
    "            print('\\tfetching {}'.format(start_date))\n",
    "            int_ = pd.interval_range(start=start_date, periods=1, freq='1d')\n",
    "            int_ = pd.Interval(int_.left[0], int_.right[0])\n",
    "            int_df = load_date(ticker, granularity, int_)\n",
    "            fname = audiolizer_temp_dir + '/{}-{}.csv.gz'.format(\n",
    "                ticker, int_.left.strftime('%Y-%m-%d'))\n",
    "            int_df.to_csv(fname, compression='gzip')\n",
    "\n",
    "    df = pd.concat(map(lambda file: pd.read_csv(file,index_col='time', parse_dates=True, compression='gzip'),\n",
    "                         fnames)).drop_duplicates()\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d9d2f6",
   "metadata": {
    "active": "ipynb"
   },
   "outputs": [],
   "source": [
    "hist = get_history('BTC-USD', '2020-04-20', '2020-05-18')\n",
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "186ed12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jupytext] Reading history.ipynb in format ipynb\r\n",
      "[jupytext] Updating history.py\r\n"
     ]
    }
   ],
   "source": [
    "!jupytext --sync history.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fe1807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
