{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a62acba",
   "metadata": {},
   "source": [
    "Objective: get_history should fetch all the data at once then save it to separate files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9457c197",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from Historic_Crypto import HistoricalData\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "granularity = int(os.environ.get('AUDIOLIZER_GRANULARITY', 300)) # seconds\n",
    "\n",
    "audiolizer_temp_dir = os.environ.get('AUDIOLIZER_TEMP', './history/')\n",
    "print('audiolizer temp data:', audiolizer_temp_dir)\n",
    "\n",
    "max_age = pd.Timedelta(os.environ.get('AUDIOLIZER_MAX_AGE', '5m'))\n",
    "print('audiolizer max daily age {}'.format(max_age))\n",
    "\n",
    "def refactor(df, frequency='1W'):\n",
    "    \"\"\"Refactor/rebin the data to a lower cadence\n",
    "\n",
    "    The data is regrouped using pd.Grouper\n",
    "    \"\"\"\n",
    "    low = df.low.groupby(pd.Grouper(freq=frequency)).min()\n",
    "    high = df.high.groupby(pd.Grouper(freq=frequency)).max()\n",
    "    close = df.close.groupby(pd.Grouper(freq=frequency)).last()\n",
    "    open_ = df.open.groupby(pd.Grouper(freq=frequency)).first()\n",
    "    volume = df.volume.groupby(pd.Grouper(freq=frequency)).sum()\n",
    "    return pd.DataFrame(dict(low=low, high=high, open=open_, close=close, volume=volume))\n",
    "\n",
    "\n",
    "def load_date(ticker, granularity, int_):\n",
    "    print('loading single date {}'.format(int_))\n",
    "    start_ = int_.left.strftime('%Y-%m-%d-%H-%M')\n",
    "    end_ = int_.right.strftime('%Y-%m-%d-%H-%M')\n",
    "    try:\n",
    "        return HistoricalData(ticker,\n",
    "                              granularity,\n",
    "                              start_,\n",
    "                              end_,\n",
    "                              ).retrieve_data()\n",
    "    except:\n",
    "        print('could not load using {} {}'.format(start_, end_))\n",
    "        raise\n",
    "\n",
    "\n",
    "def get_gaps(df, granularity):\n",
    "    new_ = refactor(df, '{}s'.format(granularity))\n",
    "    return new_[new_.close.isna()]\n",
    "\n",
    "\n",
    "def fetch_data(ticker, granularity, start_, end_):\n",
    "    \"\"\"Need dates in this format %Y-%m-%d-%H-%M\"\"\"\n",
    "    try:\n",
    "        return HistoricalData(ticker,\n",
    "                              granularity,\n",
    "                              start_,\n",
    "                              end_,\n",
    "                              ).retrieve_data()\n",
    "    except:\n",
    "        print('could not load using {} {}'.format(start_, end_))\n",
    "        raise\n",
    "\n",
    "\n",
    "def write_data(df, ticker):\n",
    "    for t, day in df.groupby(pd.Grouper(freq='1D')):\n",
    "        tstr = t.strftime('%Y-%m-%d-%H-%M')\n",
    "        fname = audiolizer_temp_dir + '/{}-{}.csv.gz'.format(\n",
    "                ticker, t.strftime('%Y-%m-%d'))\n",
    "        if len(day) > 1:\n",
    "            day.to_csv(fname, compression='gzip')\n",
    "            print('wrote {}'.format(fname))\n",
    "        \n",
    "def fetch_missing(files_status, ticker, granularity):\n",
    "    \"\"\"Iterate over batches of missing dates\"\"\"\n",
    "    for batch, g in files_status[files_status.found==0].groupby('batch', sort=False):\n",
    "        t1, t2 = g.iloc[[0, -1]].index\n",
    "        # extend by 1 day whether or not t1 == t2\n",
    "        t2 += pd.Timedelta('1D')\n",
    "        endpoints = [t.strftime('%Y-%m-%d-%H-%M') for t in [t1, t2]]\n",
    "        print('fetching {}, {}'.format(len(g), endpoints))\n",
    "        df = fetch_data(ticker, granularity, *endpoints)\n",
    "        write_data(df, ticker)\n",
    "\n",
    "        \n",
    "def get_files_status(ticker, start_date, end_date):\n",
    "    fnames = []\n",
    "    foundlings = []\n",
    "    dates = []\n",
    "    batch = []\n",
    "    batch_number = 0\n",
    "    last_found = -1\n",
    "    for int_ in pd.interval_range(start_date, end_date):\n",
    "        dates.append(int_.left)\n",
    "        fname = audiolizer_temp_dir + '/{}-{}.csv.gz'.format(\n",
    "            ticker, int_.left.strftime('%Y-%m-%d'))\n",
    "        found = int(os.path.exists(fname))\n",
    "        foundlings.append(found)\n",
    "        if found != last_found:\n",
    "            batch_number += 1\n",
    "        last_found = found\n",
    "        batch.append(batch_number)\n",
    "        fnames.append(fname)\n",
    "    files_status = pd.DataFrame(dict(files=fnames, found=foundlings, batch=batch), index=dates)\n",
    "    return files_status\n",
    "\n",
    "\n",
    "def get_today(ticker, granularity):\n",
    "    today = pd.Timestamp.now().tz_localize(None)\n",
    "    tomorrow = today + pd.Timedelta('1D')\n",
    "    start_ = '{}-00-00'.format(today.strftime('%Y-%m-%d'))\n",
    "    end_ = '{}-00-00'.format(tomorrow.strftime('%Y-%m-%d'))\n",
    "    try:\n",
    "        return HistoricalData(ticker,\n",
    "                              granularity,\n",
    "                              start_,\n",
    "                              end_,\n",
    "                              ).retrieve_data()\n",
    "    except:\n",
    "        print('could not load using {} {}'.format(start_, end_))\n",
    "        raise\n",
    "\n",
    "\n",
    "def get_age(fname):\n",
    "    \"\"\"Get the age of a given a file\"\"\"\n",
    "    st=os.stat(fname)    \n",
    "    mtime=st.st_mtime\n",
    "    return pd.Timestamp.now() - datetime.fromtimestamp(mtime)\n",
    "    \n",
    "        \n",
    "def get_history(ticker, start_date, end_date = None, granularity=granularity):\n",
    "    \"\"\"Fetch/load historical data from Coinbase API at specified granularity\n",
    "    \n",
    "    Data loaded from start_date through end of end_date\n",
    "    params:\n",
    "        start_date: (str) (see pandas.to_datetime for acceptable formats)\n",
    "        end_date: (str)\n",
    "        granularity: (int) seconds (default: 300)\n",
    "\n",
    "    price data is saved by ticker and date and stored in audiolizer_temp_dir\n",
    "    \"\"\"\n",
    "    start_date = pd.to_datetime(start_date).tz_localize(None)\n",
    "    \n",
    "    today = pd.Timestamp.now().tz_localize(None)\n",
    "    if end_date is None:\n",
    "        # don't include today\n",
    "        end_date = today # + pd.Timedelta('1D')\n",
    "    else:\n",
    "        end_date = min(today, pd.to_datetime(end_date).tz_localize(None))\n",
    "        \n",
    "    files_status = get_files_status(ticker, start_date, end_date)\n",
    "    fetch_missing(files_status, ticker, granularity)\n",
    "        \n",
    "\n",
    "    df = pd.concat(map(lambda file: pd.read_csv(file, index_col='time', parse_dates=True, compression='gzip'),\n",
    "                         files_status.files)).drop_duplicates()\n",
    "\n",
    "    if end_date == today:\n",
    "        print('end date is today!')\n",
    "        # check age of today's data. If it's old, fetch the new one\n",
    "        today_fname = audiolizer_temp_dir + '/{}-today.csv.gz'.format(ticker)\n",
    "        if os.path.exists(today_fname):\n",
    "            if get_age(today_fname) > max_age:\n",
    "                print('{} is too old, fetching new data'.format(today_fname))\n",
    "                today_data = get_today(ticker, granularity)\n",
    "                today_data.to_csv(today_fname, compression='gzip')\n",
    "            else:\n",
    "                print('{} is not that old, loading from disk'.format(today_fname))\n",
    "                today_data = pd.read_csv(today_fname, index_col='time', parse_dates=True, compression='gzip')\n",
    "        else:\n",
    "            print('{} not present. loading'.format(today_fname))\n",
    "            today_data = get_today(ticker, granularity)\n",
    "            today_data.to_csv(today_fname, compression='gzip')\n",
    "        df = pd.concat([df, today_data]).drop_duplicates()\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6107edca",
   "metadata": {
    "active": "ipynb"
   },
   "outputs": [],
   "source": [
    "hist = get_history('BTC-USD',\n",
    "                   pd.Timestamp.now().tz_localize(None)-pd.Timedelta('5D'),\n",
    "#                   pd.Timestamp.now().tz_localize(None)-pd.Timedelta('3D'),\n",
    "                  )\n",
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186ed12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupytext --sync history.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81042c6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
